# LLM Tool Interaction and History Management

This document explains how the system manages interactions with the Large Language Model (LLM) when tools (actions) are involved, particularly focusing on how message history, tool calls, and tool results are handled with the Vercel AI SDK.

## 1. Overview of Tool Usage Flow (Single Turn)

When a user sends a message that leads the LLM to use a tool, the following sequence occurs within a single invocation of `generateText` or `streamText` (from the Vercel AI SDK) in `src/services/assistant/message-handling.service.ts`:

1.  **User Input:** The user's message is received.
2.  **History Preparation:** A message history (array of `CoreMessage` objects) is prepared. Currently, this primarily includes plain text user and assistant messages.
3.  **LLM Call (Initial):** The Vercel AI SDK sends the history and the user's current message to the LLM.
4.  **LLM Decides to Use Tool:** The LLM responds indicating it wants to call one or more tools. This response includes `tool_calls` data (e.g., `[{ toolCallId: "id1", toolName: "jira_fetchTickets", args: { "projectKey": "AAD" } }]`).
5.  **Tool Execution by Backend:**
    *   The Vercel AI SDK passes these `tool_calls` to the `tools` functions provided during its setup.
    *   Our `executeFunc` wrapper (defined in `message-handling.service.ts`) for the specific tool is invoked.
    *   `executeFunc` calls `executeFunctionCall` (from `src/integrations/actions/executors.ts`).
    *   `executeFunctionCall` runs the actual action logic (which uses `executeAction`).
    *   A result (e.g., data from a successful Jira API call, or an error summary) is returned to `executeFunc`.
    *   `executeFunc` returns this result to the Vercel AI SDK.
6.  **LLM Call (With Tool Result):**
    *   The Vercel AI SDK constructs a new message of `role: 'tool'` containing the result from the executed tool, linking it via `tool_call_id`.
    *   The SDK sends the updated message history (now including the user message, the assistant's initial `tool_calls` message, and the new `tool` result message) back to the LLM.
7.  **LLM Generates Final Response:** The LLM uses the tool's result to formulate its final textual answer to the user.
8.  **Response Handling:**
    *   The `generateText` or `streamText` function resolves/streams this final text.
    *   The Vercel AI SDK also provides the `tool_calls` that occurred and the `tool_results` that were processed during this turn.

## 2. Storing Tool Interactions in MongoDB

The `handleSessionMessage` function in `src/services/assistant/message-handling.service.ts` saves the outcome of an LLM turn to the `Message` collection in MongoDB.

When a turn involves tool usage, the assistant's `Message` document is enriched:

*   **`sender`**: "assistant"
*   **`content`**: The final textual response generated by the LLM for the user.
*   **`messageType`**: This might be set to `'tool_calls'` or `'tool_results'` if such events occurred, or remain `'text'`. (Note: If both text content and tool calls exist, the `messageType` might primarily reflect the tool interaction, or this logic might need refinement to store distinct message types if necessary).
*   **`data.toolCalls`**: An array storing the `tool_calls` objects that the LLM requested during this turn.
    *   Example: `[{ toolCallId: "id1", toolName: "jira_fetchTickets", args: { "projectKey": "AAD" } }]`
*   **`data.toolResults`**: An array storing the results corresponding to those `toolCalls`.
    *   Example: `[{ toolCallId: "id1", toolName: "jira_fetchTickets", result: [ { ticket1_data }, ... ] }]`

This ensures that a record of the tool interactions is preserved in the database for auditing and potential future use.

## 3. Constructing Message History for Subsequent LLM Calls

When a new user message arrives and `handleSessionMessage` is invoked again, it fetches past messages from MongoDB to construct the history for the next LLM call.

**Current History Construction Logic:**
```typescript
// In src/services/assistant/message-handling.service.ts
const dbMessages = await getMessagesBySessionId(sessionId.toString());
const history: CoreMessage[] = dbMessages
  .filter(msg => (msg.sender === 'user' || msg.sender === 'assistant') && typeof msg.content === 'string')
  .map((msg: IMessage) => ({
    role: msg.sender as 'user' | 'assistant',
    content: msg.content as string, // Only string content is used
  }));
```

**Implications of Current Logic:**
This logic primarily reconstructs a history of plain text exchanges between the user and the assistant. It does **not** currently:
-   Explicitly look for `Message` documents that represent assistant `tool_calls` (e.g., by checking `data.toolCalls`).
-   Explicitly reconstruct `role: 'tool'` messages from `data.toolResults`.

As a result, while tool interactions *within a single turn* are handled correctly by the Vercel AI SDK's internal loop, the explicit history of *past turns'* tool calls and tool results is not passed to the LLM in subsequent turns in the Vercel AI SDK's expected structured format. The LLM would only see the final textual outcomes of those past turns.

## 4. Recommended History Construction for Full Tool Context (Vercel AI SDK)

To provide the LLM with complete context of past tool interactions across multiple turns, the history construction logic should be enhanced to map the stored `Message` documents (including those with `data.toolCalls` and `data.toolResults`) into the `CoreMessage` array format expected by the Vercel AI SDK.

**Idealized Mapping from DB `Message` to `CoreMessage[]`:**

When iterating through `dbMessages` sorted by timestamp:

1.  **User Message:**
    *   DB: `{ sender: 'user', content: "User text" }`
    *   CoreMessage: `{ role: 'user', content: "User text" }`

2.  **Assistant Message with Tool Calls:**
    *   DB: `{ sender: 'assistant', content: "Optional preliminary text", data: { toolCalls: [...] } }`
    *   CoreMessage:
        ```typescript
        {
          role: 'assistant',
          content: "Optional preliminary text", // Or null/empty if no text
          tool_calls: [/* map data.toolCalls here */]
        }
        ```
        *(Note: The Vercel AI SDK's `CoreMessage` type defines `tool_calls` as an optional top-level property on assistant messages. The `content` can be text or an array of parts including `tool_call` parts. The mapping needs to align with the SDK's specific expectation.)*

3.  **Tool Result Message(s):**
    *   For each tool call in the preceding assistant message, there should be a corresponding tool result. This might be stored in `data.toolResults` of the same assistant message, or ideally, as separate conceptual messages if the DB schema were designed for it.
    *   If `data.toolResults` are available for `data.toolCalls` on an assistant message, they need to be transformed into `role: 'tool'` messages.
    *   CoreMessage for each tool result:
        ```typescript
        {
          role: 'tool',
          tool_call_id: "id_from_tool_call", // Must match a toolCallId from the assistant's message
          // name: "tool_name_from_tool_call", // Often included or inferred by SDK
          content: "JSON string of the tool's result" // The actual data returned by the tool
        }
        ```

4.  **Final Assistant Textual Response (after tools):**
    *   DB: `{ sender: 'assistant', content: "Final text after tool use" }` (This might be the same DB message as #2 if `content` is populated after tool use, or a subsequent one).
    *   CoreMessage: `{ role: 'assistant', content: "Final text after tool use" }`

**Example of Reconstructed History for LLM:**

```json
[
  { "role": "user", "content": "Show me tickets for project X." },
  { 
    "role": "assistant", 
    "tool_calls": [{ "toolCallId": "call1", "toolName": "get_tickets", "args": {"project": "X"} }] 
  },
  { 
    "role": "tool", 
    "tool_call_id": "call1", 
    "content": "[{\"id\":\"T1\", \"summary\":\"Ticket 1\"}]" 
  },
  { "role": "assistant", "content": "Here is ticket T1: Ticket 1 summary..." },
  { "role": "user", "content": "Now show me tickets for project Y." } // New user input
]
```

**Current Status & Future Enhancement:**
-   The system **correctly executes tools and uses their results within a single LLM turn** due to the Vercel AI SDK's internal handling.
-   Tool call and result data **is saved** to the database.
-   The **enhancement opportunity** lies in the history reconstruction logic within `handleSessionMessage` to fully re-hydrate past tool interactions into the `CoreMessage[]` array for subsequent LLM calls. This would ensure the LLM has maximum context over extended conversations involving multiple tool uses across different turns. Implementing this change would involve modifying the `dbMessages.filter(...).map(...)` part of `handleSessionMessage`.

This detailed understanding helps in debugging and planning future improvements to the agent's conversational memory and tool usage capabilities.
